---
title: "Credit Card Prediction"
format: 
  html:
    css: styles.css
---

## Loding Libraries

```{r message=FALSE}
# Check which libraries are not needed
library(knitr)
library(dplyr)
library(plotly)
library(forcats)
library(ggcorrplot)
library(ggplot2)
library(ROSE)
library(caret)
library(randomForest)
library(xgboost)
library(class)
library(smotefamily)
library(corrplot)

```

## Motivation

Credit card defaults pose significant risks to financial institutions, leading to substantial losses and operational challenges. To mitigate these issues, a data science model for predicting default risk is essential. In this work, we will implement and compare four different models to help financial institutions manage credit risk more effectively using their preferred model.

## Usage

The results can be used for:

1.  Helping financial institutions to manage credit risk more effectively
2.  Guiding future data collection and risk assessment strategies.
3.  Facilitating responsible lending practices

## Questions

1.  How accurately a Machine Learning model can predict credit default risk and identify individuals as either high risk or low risk?
2.  How different models fair in predicting the credit default risk?

## Reading Data

We first read the data. This data set is provided in training and test data sets.

```{r}
test_data <- read.csv('./test_data.csv', na.strings = "")
train_data <- read.csv('./train_data.csv', na.strings = "")
```

To have a larger dataset and have a more freedome to work with the data, we cobmine the training and test sets.

```{r}
credit_card <- rbind(test_data, train_data)
```

## Data Overview

The data set is named Credit Car Prediction and downloaded from [kaggl.com](https://www.kaggle.com/datasets/tanayatipre/car-price-prediction-dataset). It is specifically created to assess the risk level for lending puproses. The dataset is a mix of continuous, categorical, and binary variables. Plus, our target variable (Is High Risk) is also a binary varible which we can use for classification modelings. Let's look at the data closely:

```{r}
cat("Dimension =", dim(credit_card),'\n')
```

As it is evident from the above output, the dataset has 20 features and 36475 observations.

THe feature list is as the following:

| Variables           | Description                                                                                                 |
|------------------------------------|------------------------------------|
| ID                  | Unique identifier for each individual                                                                       |
| Gender              | Gender of the individual (Male/Female)                                                                      |
| Has a car           | Whether the individual owns a car (Yes/No)                                                                  |
| Has a property      | Whether the individual owns property (Yes/No)                                                               |
| Children count      | Number of children the individual has                                                                       |
| Income              | Income of the individual                                                                                    |
| Employment status   | Current employment status of the individual                                                                 |
| Education level     | Highest level of education attained by the individual                                                       |
| Marital status      | Marital status of the individual                                                                            |
| Dwelling            | Type of dwelling the individual resides in                                                                  |
| Age                 | Age of the individual                                                                                       |
| Employment length   | Length of time the individual has been employed                                                             |
| Has a mobile phone  | Whether the individual has a mobile phone (Yes/No)                                                          |
| Has a work phone    | Whether the individual has a work phone (Yes/No)                                                            |
| Has a phone         | Whether the individual has a phone (Yes/No)                                                                 |
| Has an email        | Whether the individual has an email (Yes/No)                                                                |
| Job title           | Title of the individual's job                                                                               |
| Family member count | Number of family members the individual has                                                                 |
| Account age         | Age of the individual's account                                                                             |
| Is high risk        | Target variable indicating whether the individual is considered high risk for credit card eligibility (0/1) |

The head, tail, and the summary of features are as the following:

```{r}
kable(head(credit_card))
kable(tail(credit_card))
kable(summary(credit_card))
```

From the above summary, we can extract valuable insights. However, due to some features being categorical, a portion of the results are not shown. We return back to analyzing this summary again after encoding the categorical variables.

Let's check whether our dataset contains null values:

```{r}
null_val <- sapply(credit_card, function(x) sum(is.na(x)))
kable(t(null_val))
```

As we can see "Job Title" is the sole null value holder with 11323 null values which are the one third of Job Title's total values. So, we drop this column entirely. This is one of the preprocessing steps. However, to make our data visualizations more convenient, we removed the null value holders sooner.

```{r}
credit_card <- select(credit_card, -Job.title)
```

## Data Virtualization And Preprocessing

In this section, we will visualize the outcome (Is High Risk) and the features that seem most relevant to the outcome as well as preprocessing the data. The preprocessing steps are including handling missing values, feature selection, encoding catagorical variables,handling imbalanced data, spliting the dataset into training and test sets, standardizing the data.

```{r}
par(mfrow = c(2, 2))

counts <- table(credit_card$Is.high.risk)
barplot(counts,
        main = "Distribution of Risk",
        xlab = "Risk",
        ylab = "Frequency",
        col = "lightblue",
        names.arg = c("Low Risk", "High Risk"))

hist(credit_card$Employment.length,
        main = "Distributtion of Employment Length",
        xlab = "Employment Length",
        ylab = "Frequency",
        col = "lightblue")

hist(credit_card$Age,
        main = "Distributtion of Age",
        xlab = "Age",
        ylab = "Frequency",
        col = "lightblue")

hist(credit_card$Account.age,
        main = "Distribution of Account Age",
        xlab = "Age",
        ylab = "Frequency",
        col = "lightblue")
```

From these figures we can understand:

1.  The target variable (Is High Risk) is highly unbalanced. We need to take care of it before implementing our models.
2.  we have two long bars in the distribution of employment length figure. This is indicating that most people are new to thir jobs with and a portion of indivisuals have had their jobs for a quite long time. All other employment lengths have a similar frequencies.
3.  The distribution of age chart is mostly a normal distribution with some skews at the right side of it. These are indicating that most applicants are middle aged and are placed in the range of 41 to 49 years of age.
4.  The distribution of account age shows that the data is more skewed on the right side meaning most accounts are new.

```{r}
boxplot(credit_card$Income ~ credit_card$Gender,
main="Income Distribution VS Gender",
xlab = "Gender",
ylab = "Income")
```

This boxplot is suggesting while male applicant are getting higher income on average, there is not much of difference between average income of males and females.

```{r}
credit_card$Education.level <- factor(credit_card$Education.level,
                                     levels = c("Secondary / secondary special", "Higher education", "Incomplete higher", "Lower secondary", "Academic degree"),
                                     labels = c("Sec. Spec.", "Higher Edu.", "Incomplete", "Lower Sec.", "Academic"))
boxplot(credit_card$Income ~ credit_card$Education.level,
main="Income Distribution VS Education Level",
xlab = "Education Level",
ylab = "Income")
```

The income distribution versus education level boxplot shows that with a better education level, people are getting better incomes on average. However, there are some exceptions. For instance, we can see quite a few outliers for higher education which indicates that some people who has higher education receive better income compared to applicans with other level of education. Plus, we can see although the median of the income for academic degree is above all other education levels, its variability is less than them with no outliers at all.

```{r}
# Check label
credit_card$Marital.status <- factor(credit_card$Marital.status,
                                     levels = c("Civil marriage", "Married", "Separated", "Single / not married", "Widow"),
                                     labels = c("Civil", "Married", "Sep.", "Single", "Widow"))
boxplot(credit_card$Age~ credit_card$Marital.status,
main="Age Distribution VS Marital Status",
xlab = "Marital Status",
ylab = "Age")
```

This boxplot is indicating that single people consitute a wider range of age with mostly young people, and widows are mosly old with some younger outliers. All other marital statuses are place between these two.

```{r}
colorscale <- list(
  list(0, 'blue'),
  list(1, 'orange')
)

fig <- plot_ly(
  type = 'splom',
  dimensions = list(
    list(label = 'Income', values = credit_card$Income),
    list(label = 'Age', values = credit_card$Age),
    list(label = 'Account age', values = credit_card$Account.age)
  ),
  marker = list(
    color = credit_card$Is.high.risk,
    colorscale = colorscale,
    showscale = TRUE
  )
)

fig <- fig %>% layout(
  title = 'Pairplot of Select Features',
  dragmode = 'select',
  hovermode = 'closest'
)

fig
```

The pairplot shows a strong positive correlation between income and account age, while income and age, as well as account age and age, show scattered distributions with no clear linear relationship. Plus, the data contains some outliers, particularly in income.

Now, we try to encode the categorical columns.

```{r}
columns_to_encode <- c('Gender', 'Has.a.car', 'Has.a.property', 'Employment.status', 
                       'Education.level', 'Marital.status', 'Dwelling', 'Family.member.count')

# Encode each column
for (column in columns_to_encode) {
  credit_card[[column]] <- as.numeric(factor(credit_card[[column]]))
}

kable(head(credit_card))
```

Let's see the summary of features again and extract insights from it.

```{r}
kable(summary(credit_card))
```

The summary reveals that 33% of applicants are male, 38% have a car, they are getting 186k on average, and the average account age is 26 months.

To improve model performance, simplifying the model, and enhnace the data quality (e.g. removing the noise), we will do feature selection based on the correclation matrix. To start this process, we draw the correclation plot and remove the variables that are not correlated with any other features, specially the target variable. We also remove one of two variables that are highly correlated with each other.

```{r}
library(corrplot)
library(dplyr)
numeric <- select(credit_card, -Has.a.mobile.phone)
correlation_matrix <- cor(numeric)
corrplot(correlation_matrix, method = "circle")
```

By analyzing the above correlation matrix, we decided to remove "ID", "Has a mobile Phone", "Has a Phone", "Children Count", and "Age".

```{r}
credit_card_all <- select(credit_card, -Has.a.mobile.phone)
credit_card <- select(credit_card, -ID, -Has.a.mobile.phone, -Has.a.phone, -Children.count, -Age)
```

After this process the correlation matrix, and our dataset will be turned into the following figures. As you can see, there is no more high correlation between features of the dataset.

```{r}

correlation_matrix <- cor(credit_card)
corrplot(correlation_matrix, method = "circle")
```

```{r}
kable(head(credit_card))
```

Now, we devide our dataset into the feature and the traget variable.

```{r}
X <- select(credit_card, -Is.high.risk)
y <- select(credit_card, Is.high.risk)

X_all <- select(credit_card_all, -Is.high.risk)
y_all <- select(credit_card_all, Is.high.risk)
```

The head of the feature variables dataset is as teh following:

```{r}
kable(head(X))
```

For the convenience, let's see the table of the target variable.

```{r}
print(table(y))
```

The output above reveals that the target variable is highly unbalanced. The following distribution plot shows it better:

```{r}
#| warning: false
ggplot(y, aes(x = `Is.high.risk`)) +
  geom_histogram(stat = "count", fill = "blue", color = "black") +
  ggtitle("Distribution of High Risk Individuals") +
  xlab("Is High Risk") +
  ylab("Count") +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 15),
    plot.margin = unit(c(1, 1, 1, 1), "cm")
  )
```

To solve this issue, we are using ADAS (Adaptive Synthetic). It is a technique derived from SMOTE and is designed to address class imbalance by generating synthetic minority class examples. To give our data more variability, we shuffled the data after this step. This way, we decreased the possibility of our model to learn from the order of the observations.

```{r}
#| warning: false

# For the feature selected dataset
set.seed(10)
adasyn_data <- ADAS(X, y, K = 5)
balanced_data <- adasyn_data$data
names(balanced_data)[names(balanced_data) == "class"] <- "Is.high.risk"
balanced_data <- balanced_data[sample(nrow(balanced_data)), ]
resampled_data <- balanced_data
kable(head(resampled_data))
table(resampled_data$Is.high.risk)


# For the original dataset
adasyn_data <- ADAS(X_all, y_all, K = 5)
balanced_data <- adasyn_data$data
names(balanced_data)[names(balanced_data) == "class"] <- "Is.high.risk"
balanced_data <- balanced_data[sample(nrow(balanced_data)), ]
resampled_data_all <- balanced_data
```

This table output suggest that the target variable is completely balanced now.

Now, we draw the distribution of high risk indivisuals plot again to see how the data has gotten balanced with our own eyes too.

```{r}
#| warning: false

# Feature selected
X_resampled<- select(resampled_data, -Is.high.risk)
y_resampled <- select(resampled_data, Is.high.risk)

# Original Data
X_resampled_all<- select(resampled_data_all, -Is.high.risk)
y_resampled_all <- select(resampled_data_all, Is.high.risk)

ggplot(y_resampled, aes(x = `Is.high.risk`)) +
  geom_histogram(stat = "count", fill = "blue", color = "black") +
  ggtitle("Distribution of High Risk Individuals") +
  xlab("Is High Risk") +
  ylab("Count") +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 15),
    plot.margin = unit(c(1, 1, 1, 1), "cm")
  )
```

The next step is splitting the dataset into the training and test sets. We are using a split of 70% for this purpose.

```{r}
#| warning: false

train_size <- 0.7
# Feature selected data
trainIndex <- createDataPartition(resampled_data$Is.high.risk, p = train_size, list = FALSE)

X_train <- resampled_data[trainIndex, -ncol(resampled_data)]
y_train <- resampled_data[trainIndex, ncol(resampled_data), drop=FALSE]
X_test <- resampled_data[-trainIndex, -ncol(resampled_data)]
y_test <- resampled_data[-trainIndex, ncol(resampled_data), drop=FALSE]

# Original data
trainIndex_all <- createDataPartition(resampled_data_all$Is.high.risk, p = train_size, list = FALSE)

X_train_all <- resampled_data_all[trainIndex_all, -ncol(resampled_data_all)]
y_train_all <- resampled_data_all[trainIndex_all, ncol(resampled_data_all), drop=FALSE]
X_test_all <- resampled_data_all[-trainIndex_all, -ncol(resampled_data_all)]
y_test_all <- resampled_data_all[-trainIndex_all, ncol(resampled_data_all), drop=FALSE]
```

To be assured about all the features contribute equally to the models, we scale the each feature using its specific mean and standard deviation.

```{r}
# Selected features data
X_train <- scale(X_train)
train_mean <- attr(X_train, "scaled:center")
train_sd <- attr(X_train, "scaled:scale")
X_test <- scale(X_test, center = train_mean, scale = train_sd)

cat('Shape for training data: (', paste(dim(X_train), collapse = ', '), '), (', paste(dim(y_train), collapse = ', '), ')', '\n')
cat('Shape for testing data: (', paste(dim(X_test), collapse = ', '), '), (', paste(dim(y_test), collapse = ', '), ')', '\n')
# Original data
X_train_all <- scale(X_train_all)
train_mean_all <- attr(X_train_all, "scaled:center")
train_sd_all <- attr(X_train_all, "scaled:scale")
X_test_all <- scale(X_test_all, center = train_mean_all, scale = train_sd_all)
```

## Models

We are using the following four models for this project:

-   Logistic Regression
-   Random Forest Classification
-   XGBoost Classification
-   KNN Classification

For each model, we will use two datasets. The first dataset is the dataset with all the predicators and the second one is the dataset which we removed a portion of the origianl predicators in the last section. The goal is to determine which model performs best for our specific question. The parameters for each model are optimized through fine-tuning and exploring various combinations.

### Logistic Regression

**Logistic Regression with all the features:**

```{r}
#| warning: false

# Combine X_train_all and y_train_all into a single data frame for training

trainData_all <- cbind(X_train_all, Is.high.risk = y_train_all)
testData_all <- cbind(X_test_all, Is.high.risk = y_test_all)

trainData_all$Is.high.risk <- as.factor(trainData_all$Is.high.risk)

ctrl <- trainControl(method = "cv", number = 5)

fit_cv <- train(Is.high.risk ~ ., 
                data = trainData_all,
                method = "glm",
                family = "binomial",
                trControl = ctrl)

summary(fit_cv$finalModel)
acc_cv <- max(fit_cv$results$Accuracy)
cat("Cross-validated Accuracy:", acc_cv, "\n")

test_preds <- predict(fit_cv, newdata = testData_all)

conf_test <- table(Predicted = test_preds, Actual = testData_all$Is.high.risk)
print(conf_test)

acc_logistic_all <- mean(test_preds == testData_all$Is.high.risk)
cat("Test Accuracy:", acc_logistic_all, "\n")
```

The summary above shows that three predictors have p-values greater than 0.05. This means there is no clear evidence that these predictors are statistically significant. So, their estimates for the response variable cannot be trusted. Plus, the accuracy score for logistic regression is not that good which makes it not an optimal model to predict our future data points.

**Logistic Regression with selected features:**

```{r}
#| warning: false

# Combine X_train and y_train into a single data frame for training
trainData <- cbind(X_train, Is.high.risk = y_train)
testData <- cbind(X_test, Is.high.risk = y_test)

trainDataLogestic <- trainData

trainDataLogestic$Is.high.risk <- as.factor(trainDataLogestic$Is.high.risk)

ctrl <- trainControl(method = "cv", number = 5)

fit_cv <- train(Is.high.risk ~ ., 
                data = trainData,
                method = "glm",
                family = "binomial",
                trControl = ctrl)

summary(fit_cv$finalModel)
acc_cv <- max(fit_cv$results$Accuracy)
cat("Cross-validated Accuracy:", acc_cv, "\n")

test_preds <- predict(fit_cv, newdata = testData)

conf_test <- table(Predicted = test_preds, Actual = testData$Is.high.risk)
print(conf_test)

acc_logistic <- mean(test_preds == testData$Is.high.risk)
cat("Test Accuracy:", acc_logistic, "\n")
```

The above summary shows that close to all of the predictors are statistically significant. Also, the accuracy of the model containing all the variables and the model with selected features are verey similar. These suggest that for the sake of simplifying the model and enhancing the data quality, there is no issue to fit our model with our selected features.

### Random Forest Classifier

**Random Forest Classifier with all the features:**

```{r}

trainData_all$Is.high.risk <- as.factor(trainData_all$Is.high.risk)
testData_all$Is.high.risk <- as.factor(testData_all$Is.high.risk)

ctrl <- trainControl(method = "cv", number = 5)

set.seed(123)
fit_cv_rf <- train(
  Is.high.risk ~ .,
  data = trainData_all,
  method = "rf",
  tuneGrid = expand.grid(mtry = 1:5),
  trControl = ctrl,
  ntree = 300,
  importance = TRUE
)

print(fit_cv_rf)
cat("Cross-validated Accuracy:", fit_cv_rf$results$Accuracy, "\n")
```

Metircs of the model for the test set:

```{r}
rf_predictions_test <- predict(fit_cv_rf, newdata = testData_all)

conf_test <- table(Predicted = rf_predictions_test, Actual = testData_all$Is.high.risk)
print(conf_test)

acc_rf_all <- mean(rf_predictions_test == testData_all$Is.high.risk)
cat("Test Accuracy:", acc_rf_all, "\n")

```

**Random Forest Classifier with selected features:**

```{r}

trainData$Is.high.risk <- as.factor(trainData$Is.high.risk)
testData$Is.high.risk <- as.factor(testData$Is.high.risk)

ctrl <- trainControl(method = "cv", number = 5)

set.seed(123)
fit_cv_rf <- train(
  Is.high.risk ~ .,
  data = trainData,
  method = "rf",
  tuneGrid = expand.grid(mtry = 1:5),
  trControl = ctrl,
  ntree = 300,
  importance = TRUE
)

print(fit_cv_rf)
cat("Cross-validated Accuracy:", fit_cv_rf$results$Accuracy, "\n")
```

Metircs of the model for the test set:

```{r}
rf_predictions_test <- predict(fit_cv_rf, newdata = testData)

conf_test <- table(Predicted = rf_predictions_test, Actual = testData$Is.high.risk)
print(conf_test)

acc_rf <- mean(rf_predictions_test == testData$Is.high.risk)
cat("Test Accuracy:", acc_rf, "\n")
```

The Random Forest Classifier achieved an impressive accuracy of 99%, substantially outperforming the Logistic Regression model, which reached only 62%. This significant performance gap highlights the superiority of Random Forest in capturing the underlying patterns of the data. Additionally, the accuracy of the Random Forest model remained consistent whether using all features or the selected subset, indicating that feature selection does not compromise performance. Therefore, for the sake of simplifying the model and enhancing data quality, using the selected features is a justified and effective approach.

### XGBoost Classifier

**XGBoost Classifier with all the features:**

```{r}
# Train the model using the xgboost function
dtrain <- xgb.DMatrix(data = X_train_all, label = y_train_all$Is.high.risk)

params <- list(
  objective = "binary:logistic",
  eta = 0.03,
  max_depth = 2,
  subsample = 0.8,
  colsample_bytree = 0.9,
  eval_metric = "auc"
)

set.seed(123)
cv <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 1000,
  nfold = 5,
  early_stopping_rounds = 50,
  verbose = 0
)

best_nrounds <- cv$best_iteration
cat("Best number of boosting rounds:", best_nrounds, "\n")

fit <- xgboost(
  data = dtrain,
  label = y_train_all$Is.high.risk,
  params = params,
  nrounds = best_nrounds,
  verbose = 0
)

xgb_pred_train <- ifelse(predict(fit, X_train_all) > 0.6, 1, 0)
xgb_pred_test  <- ifelse(predict(fit, X_test_all) > 0.6, 1, 0)

cat("\nTraining Accuracy:\n")
conf_train <- table(Predicted = xgb_pred_train, Actual = trainData_all$Is.high.risk)
print(conf_train)
acc_train <- mean(xgb_pred_train == trainData_all$Is.high.risk)
cat("Training Accuracy:", acc_train, "\n")

cat("\nTest Accuracy:\n")
conf_test <- table(Predicted = xgb_pred_test, Actual = testData_all$Is.high.risk)
print(conf_test)
acc_xgb_all <- mean(xgb_pred_test == testData_all$Is.high.risk)
cat("Test Accuracy:", acc_xgb_all, "\n")

importance_matrix <- xgb.importance(feature_names = colnames(X_train_all), model = fit)
xgb.plot.importance(importance_matrix)

```

**XGBoost Classifier with selected features:**

```{r}
# Train the model using the xgboost function

dtrain <- xgb.DMatrix(data = X_train, label = y_train$Is.high.risk)

params <- list(
  objective = "binary:logistic",
  eta = 0.03,
  max_depth = 2,
  subsample = 0.8,
  colsample_bytree = 0.9,
  eval_metric = "auc"
)

set.seed(123)
cv <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 1000,
  nfold = 5,
  early_stopping_rounds = 50,
  verbose = 0
)

best_nrounds <- cv$best_iteration
cat("Best number of boosting rounds:", best_nrounds, "\n")

fit <- xgboost(
  data = dtrain,
  label = y_train$Is.high.risk,
  params = params,
  nrounds = best_nrounds,
  verbose = 0
)

xgb_pred_train <- ifelse(predict(fit, X_train) > 0.6, 1, 0)
xgb_pred_test  <- ifelse(predict(fit, X_test) > 0.6, 1, 0)

cat("\nTraining Accuracy:\n")
conf_train <- table(Predicted = xgb_pred_train, Actual = trainData$Is.high.risk)
print(conf_train)
acc_train <- mean(xgb_pred_train == trainData$Is.high.risk)
cat("Training Accuracy:", acc_train, "\n")

cat("\nTest Accuracy:\n")
conf_test <- table(Predicted = xgb_pred_test, Actual = testData$Is.high.risk)
print(conf_test)
acc_xgb <- mean(xgb_pred_test == testData$Is.high.risk)
cat("Test Accuracy:", acc_xgb, "\n")

importance_matrix <- xgb.importance(feature_names = colnames(X_train), model = fit)
xgb.plot.importance(importance_matrix)
```

The XGBoost Classifier achieved a strong accuracy of 93% using all features and 91% with selected features, indicating its ability to effectively capture complex relationships in the data. While it does not outperform the Random Forest model, it significantly surpasses Logistic Regression, making it a more suitable choice for our task. Moreover, the slight drop in performance when using selected features suggests that the simplified model retains most of the predictive power. The feature importance plots further support this by showing that the removed variables contribute minimally, justifying the use of the reduced feature set for a more streamlined and interpretable model.

### KNN Classifier

**KNN Classifier with all the features:**

```{r}
# Prepare target variable
y <- as.factor(y_train_all$Is.high.risk)

k_values <- 3:13
mean_accuracies <- numeric(length(k_values))

set.seed(123)
folds <- createFolds(y, k = 5, returnTrain = FALSE)

for (j in seq_along(k_values)) {
  k <- k_values[j]
  accs <- numeric(length(folds))

  for (i in seq_along(folds)) {
    val_idx <- folds[[i]]
    train_idx <- setdiff(seq_along(y), val_idx)

    X_train_cv <- X_train_all[train_idx, , drop = FALSE]
    y_train_cv <- y[train_idx]
    X_val_cv   <- X_train_all[val_idx, , drop = FALSE]
    y_val_cv   <- y[val_idx]

    pred_cv <- knn(train = X_train_cv, test = X_val_cv, cl = y_train_cv, k = k)
    accs[i] <- mean(pred_cv == y_val_cv)
  }

  mean_accuracies[j] <- mean(accs)
  cat(sprintf("k = %d, Mean CV Accuracy = %.4f\n", k, mean_accuracies[j]))
}

best_k_index <- which.max(mean_accuracies)
best_k <- k_values[best_k_index]
cat(sprintf("Best k = %d with CV Accuracy = %.4f\n", best_k, mean_accuracies[best_k_index]))

plot(k_values, mean_accuracies, type = "b", col = "blue", pch = 19,
     xlab = "k (Number of Neighbors)", ylab = "Mean CV Accuracy",
     main = "KNN Cross-Validation Accuracy vs. k")

y_test_factor <- as.factor(y_test_all$Is.high.risk)
fit_knn_test <- knn(train = X_train_all, test = X_test_all, cl = y, k = best_k)

conf_test <- table(Predicted = fit_knn_test, Actual = y_test_factor)
print(conf_test)

acc_knn_all <- mean(fit_knn_test == y_test_factor)
cat(sprintf("Test Accuracy (k = %d): %.4f\n", best_k, acc_knn_all))

```

**KNN Classifier with selected features:**

```{r}
# Prepare target variable
y <- as.factor(y_train$Is.high.risk)

k_values <- 3:13
mean_accuracies <- numeric(length(k_values))

set.seed(123)
folds <- createFolds(y, k = 5, returnTrain = FALSE)

for (j in seq_along(k_values)) {
  k <- k_values[j]
  accs <- numeric(length(folds))

  for (i in seq_along(folds)) {
    val_idx <- folds[[i]]
    train_idx <- setdiff(seq_along(y), val_idx)

    X_train_cv <- X_train[train_idx, , drop = FALSE]
    y_train_cv <- y[train_idx]
    X_val_cv   <- X_train[val_idx, , drop = FALSE]
    y_val_cv   <- y[val_idx]

    pred_cv <- knn(train = X_train_cv, test = X_val_cv, cl = y_train_cv, k = k)
    accs[i] <- mean(pred_cv == y_val_cv)
  }

  mean_accuracies[j] <- mean(accs)
  cat(sprintf("k = %d, Mean CV Accuracy = %.4f\n", k, mean_accuracies[j]))
}

# Determine the best k
best_k_index <- which.max(mean_accuracies)
best_k <- k_values[best_k_index]
cat(sprintf("Best k = %d with CV Accuracy = %.4f\n", best_k, mean_accuracies[best_k_index]))

plot(k_values, mean_accuracies, type = "b", col = "blue", pch = 19,
     xlab = "k (Number of Neighbors)", ylab = "Mean CV Accuracy",
     main = "KNN Cross-Validation Accuracy vs. k")

y_test_factor <- as.factor(y_test$Is.high.risk)
fit_knn_test <- knn(train = X_train, test = X_test, cl = y, k = best_k)

conf_test <- table(Predicted = fit_knn_test, Actual = y_test_factor)
print(conf_test)

acc_knn <- mean(fit_knn_test == y_test_factor)
cat(sprintf("Test Accuracy (k = %d): %.4f\n", best_k, acc_knn))
```

The K-Nearest Neighbors (KNN) model demonstrated excellent performance, achieving 98% accuracy with all features and 97% with the selected subset. This places it just behind the Random Forest model, while still outperforming both XGBoost and Logistic Regression. The minimal difference in performance between the full and reduced feature sets suggests that most predictive information is retained after feature selection. Given its simplicity and strong performance, KNN stands out as an effective and interpretable model for this task, particularly when using the streamlined dataset.

## Results

```{r}
library(ggplot2)

model_names <- c(
  'LR Selected', 'LR Original',
  'RF Selected', 'RF Original',
  'XGB Selected', 'XGB Original',
  'KNN Selected', 'KNN Original'
)

accuracy_scores <- c(
  round(acc_logistic, 2) * 100,
  round(acc_logistic_all, 2) * 100,
  round(acc_rf, 2) * 100,
  round(acc_rf_all, 2) * 100,
  round(acc_xgb, 2) * 100,
  round(acc_xgb_all, 2) * 100,
  round(acc_knn, 2) * 100,
  round(acc_knn_all, 2) * 100
)

data <- data.frame(
  Model = factor(model_names, levels = model_names),
  Accuracy_Score = accuracy_scores
)

colors <- c(
  'LR Selected' = '#E7B8B8',
  'LR Original' = '#F7C8C8',
  'RF Selected' = '#B8E7B8',
  'RF Original' = '#C8F7C8',
  'XGB Selected' = '#B8B8E7',
  'XGB Original' = '#C8C8F7',
  'KNN Selected' = '#E7E7B8',
  'KNN Original' = '#F7F7C8'
)

fig <- ggplot(data, aes(x = Model, y = Accuracy_Score, fill = Model)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(Accuracy_Score, "%")), vjust = -0.5, size = 6) +  
  scale_fill_manual(values = colors) +
  labs(
    title = "Comparison of Model Accuracy",
    x = "Model",
    y = "Accuracy Score"
  ) +
  theme_minimal(base_size = 18) +  # Set base font size
  theme(
    plot.title = element_text(hjust = 0.5, size = 24, face = "bold"),  
    axis.text.x = element_text(angle = 45, hjust = 1, size = 16),      
    axis.text.y = element_text(size = 16),                             
    axis.title.x = element_text(size = 18),                            
    axis.title.y = element_text(size = 18),                            
    legend.position = "none",
    plot.margin = unit(c(0, 1, 1, 1), "cm")                           
  ) +
  coord_cartesian(ylim = c(0, 119))

print(fig)

```

As illustrated in the figure above, the Random Forest Classifier consistently outperforms other models, achieving 99% accuracy with both the original and reduced feature sets. The K-Nearest Neighbors (KNN) model follows closely, with 98% and 97% accuracy, respectively. XGBoost also performs robustly, yielding 93% accuracy on the selected features and 91% on the original. In contrast, Logistic Regression lags behind, reaching only 63% with selected features and 62% with all features.

These results directly address our research questions. For first question, the tuned Random Forest model demonstrates that a well-optimized machine learning approach can predict credit default risk with up to 99% accuracy, indicating high reliability in distinguishing between high- and low-risk individuals. Regarding second question, the comparative evaluation shows that ensemble models like Random Forest and XGBoost significantly outperform baseline models like Logistic Regression, with KNN also offering strong results. The close alignment between results on full and reduced feature sets further supports the effectiveness of our preprocessing and feature selection pipeline in preserving predictive performance while improving model simplicity.

## Impact

There are two major impacts for this work: 1. The insights gained from the model can help financial institutions manage credit risk more effectively, enabling better decision-making in loan approvals. 2. These findings can guide future data collection and risk assessment strategies, ensuring more accurate predictions and improved financial stability.

## Limitations

This work has three major limiations:

1.  The sample size is very small. So, even if the accuracy of our model is high enough, it might not cover all applicants which limits the model's generalizability. A larger sample size is suggested for similar works.

2.  Some important features, like credit score, are missing from the dataset. This might result in incomplete insights, affecting the models' accuracy and reliability. It is better for the model to incorporate additional features in it.

3.  The models were trained on historical data. So, they might not capture future trends of data. As a result, we are suggesting to implement the models in a real-world setting and continuously monitor rheir performance.

## Conclusion

In this work, we classified credit card borrowers as high risk or low risk, using four Machine Learning models of Logistic Regression, Random Forest Classfier, XGBoost Classifier, and KNN Classifier. We merged the original training and test sets, handled missing values, encoded categorical variables, and balanced the target variable. After some virtualizations to help us interpret the model better, we applied our models and evaluated them using metrics such as confusion matrix, accuracy, recall and precision. The results proved that Random Forest outperforms other models with the highest accuracy. So, for this work Random Forest will be our suggested model for predicting credit default risk assessment.
